# 股票数据爬虫架构设计文档

# 1. 项目概述

本项目是一个股票数据采集系统，用于定期爬取股票相关数据并存储到数据库。系统采用 ETL Pipeline 架构，将数据处理流程分为采集（Extract）、转换（Transform）、加载（Load）三个独立阶段。

## 1.1 核心特性

- 多数据源支持（Tushare、Akshare等）

- 模块化的 Pipeline 设计

- 配置驱动的数据处理流程

- 支持增量更新和全量更新

- 完善的错误处理和重试机制

## 1.2 数据类型

- 日K线数据 (daily_kline)

- 复权因子 (adj_factor)

- 股票基本信息 (basic_info)

- 交易日历 (trade_calendar)

- 除权除息日 (ex_date) - 仅作为中间数据

# 2. 架构设计

## 2.1 整体架构

```text

┌─────────────┐     ┌──────────────┐     ┌────────────┐
│  Collector  │ --> │ Transformer  │ --> │   Loader   │
└─────────────┘     └──────────────┘     └────────────┘
      ↓                    ↓                    ↓
   数据采集             数据清洗转换          数据入库
```

## 2.2 目录结构

```text

stock-data-pipeline/
├── collectors/              # 数据采集层
│   ├── base.py             # 采集器基类
│   ├── daily_kline.py      # K线数据采集
│   ├── adj_factor.py       # 复权因子采集
│   ├── ex_date.py          # 除权除息日采集（内部组件）
│   ├── basic_info.py       # 基本信息采集
│   └── trade_calendar.py   # 交易日历采集
│
├── transformers/            # 数据转换层
│   ├── base.py             # 转换器基类
│   ├── daily_kline.py      # K线数据清洗、标准化
│   ├── adj_factor.py       # 复权因子处理
│   ├── basic_info.py       # 基本信息处理
│   └── trade_calendar.py   # 交易日历处理
│
├── loaders/                 # 数据加载层
│   ├── base.py             # 加载器基类
│   ├── daily_kline.py      # K线入库逻辑
│   ├── adj_factor.py       # 复权因子入库逻辑
│   ├── basic_info.py       # 基本信息入库逻辑
│   └── trade_calendar.py   # 交易日历入库逻辑
│
├── pipelines/               # 流水线编排层
│   ├── base.py             # Pipeline基类
│   ├── daily_pipeline.py   # 每日更新流水线
│   ├── history_pipeline.py # 历史数据补全流水线
│   └── realtime_pipeline.py# 实时数据流水线
│
├── models/                  # 数据模型
│   ├── kline.py            # K线模型
│   ├── factor.py           # 复权因子模型
│   ├── stock.py            # 股票基本信息模型
│   └── calendar.py         # 交易日历模型
│
├── orchestrator/            # 调度编排层
│   ├── scheduler.py        # 任务调度器
│   ├── dependency.py       # 任务依赖管理
│   └── monitor.py          # 监控和告警
│
├── config/                  # 配置管理
│   ├── pipelines/          # Pipeline配置
│   │   ├── daily_kline.yaml
│   │   └── adj_factor.yaml
│   ├── sources.yaml        # 数据源配置
│   ├── schedules.yaml      # 调度配置
│   └── database.yaml       # 数据库配置
│
└── common/                  # 公共组件
    ├── exceptions.py       # 异常定义
    ├── validators.py       # 数据验证
    └── utils.py           # 工具函数
```

# 3. 核心组件设计

## 3.1 Collector（采集器）

**职责**：从外部数据源采集原始数据

```python

class BaseCollector(ABC):
    @abstractmethod
    def collect(self, params: Dict[str, Any]) -> pd.DataFrame:
        """采集数据的核心方法"""
        pass
```

**设计要点**：

- 支持多数据源切换

- 内置重试机制

- 参数验证

- 统一的数据返回格式（DataFrame）

## 3.2 Transformer（转换器）

**职责**：数据清洗、标准化、衍生指标计算

```python

class BaseTransformer(ABC):
    @abstractmethod
    def transform(self, data: pd.DataFrame) -> pd.DataFrame:
        """转换数据"""
        pass
```

**主要功能**：

- 字段重命名和映射

- 数据类型转换

- 异常值处理

- 缺失值处理

- 衍生指标计算

- 数据质量验证

## 3.3 Loader（加载器）

**职责**：将处理后的数据持久化到数据库

```python

class BaseLoader(ABC):
    @abstractmethod
    def load(self, data: pd.DataFrame) -> None:
        """加载数据到数据库"""
        pass
```

**加载策略**：

- append：追加数据

- replace：替换数据

- upsert：存在则更新，不存在则插入

## 3.4 Pipeline（流水线）

**职责**：编排 ETL 流程

```python

class DailyKlinePipeline:
    def run(self, stock_codes: List[str], start_date: str, end_date: str):
        # 1. Extract - 采集
        raw_data = self.collector.collect(params)
        
        # 2. Transform - 转换
        clean_data = self.transformer.transform(raw_data)
        
        # 3. Load - 加载
        self.loader.load(clean_data)
```

# 4. 数据依赖处理

## 4.1 依赖关系示例

复权因子（adj_factor）依赖除权除息日（ex_date）的处理方式：

```python

class AdjFactorCollector(BaseCollector):
    def __init__(self, config):
        super().__init__(config)
        # 组合模式：内部使用 ExDateCollector
        self.ex_date_collector = ExDateCollector(config)
    
    def collect(self, params):
        # 1. 获取除权除息日
        ex_dates = self.ex_date_collector.get_ex_dates_list(stock_code)
        
        # 2. 基于除权除息日采集复权因子
        adj_factors = self._collect_adj_factors(stock_code, ex_dates)
        
        return adj_factors
```

## 4.2 设计原则

- 独立的 Collector 类：即使数据不入库，也实现独立的采集器

- 组合优于继承：通过组合方式处理依赖关系

- 明确的依赖声明：在配置文件中声明任务依赖

# 5. 配置管理

## 5.1 Pipeline 配置示例

```yaml

# config/pipelines/daily_kline.yaml
daily_kline:
  collector:
    source: tushare
    token: ${TUSHARE_TOKEN}
    retry_times: 3
    timeout: 30
    
  transformer:
    transform_rules:
      remove_halted: true      # 剔除停牌数据
      handle_split: true       # 处理拆股
      validate_ohlc: true      # 验证OHLC关系
      
  loader:
    table: daily_klines
    load_strategy: upsert      # append/replace/upsert
    upsert_keys: ['stock_code', 'date']
    batch_size: 1000
```

## 5.2 调度配置

```yaml

# config/schedules.yaml
pipelines:
  daily_update:
    - name: trade_calendar
      schedule: "0 9 * * *"
      
    - name: basic_info
      schedule: "0 9:30 * * *"
      depends_on: [trade_calendar]
      
    - name: daily_kline
      schedule: "0 16 * * *"
      depends_on: [trade_calendar, basic_info]
      
    - name: adj_factor
      schedule: "30 16 * * *"
      depends_on: [daily_kline]
```

# 6. 任务调度

## 6.1 调度策略

- 定时调度：使用 cron 表达式定义执行时间

- 依赖管理：确保任务按正确顺序执行

- 失败重试：自动重试失败的任务

- 并发控制：控制同时执行的任务数量

## 6.2 执行模式

```python

# 增量更新（每日）
pipeline.run_incremental(stock_codes, days_back=1)

# 全量更新（初始化或修复）
pipeline.run_full(stock_codes)

# 指定日期范围
pipeline.run(stock_codes, start_date='2024-01-01', end_date='2024-01-31')
```

# 7. 错误处理

## 7.1 错误处理策略

### Collector 层

- 网络异常：自动重试

- 数据源异常：切换备用数据源

- 参数错误：快速失败

### Transformer 层

- 数据异常：记录日志，跳过或修复

- 验证失败：根据配置决定是否继续

### Loader 层

- 数据库连接异常：重试

- 约束冲突：使用 upsert 策略

- 事务失败：回滚并重试

## 7.2 监控告警

- 任务执行状态监控

- 数据质量监控

- 性能指标监控

- 异常告警通知

# 8. 扩展性设计

## 8.1 添加新的数据类型

1. 创建对应的 Collector、Transformer、Loader

2. 添加数据模型

3. 创建 Pipeline

4. 配置调度任务

## 8.2 添加新的数据源

1. 在 Collector 中实现新数据源的采集逻辑

2. 在配置文件中添加数据源配置

3. 无需修改其他组件

# 9. 最佳实践

## 9.1 设计原则

- 单一职责：每个组件只负责一件事

- 依赖倒置：依赖抽象而非具体实现

- 开闭原则：对扩展开放，对修改关闭

- DRY原则：避免重复代码

## 9.2 开发建议

- 配置驱动：尽量通过配置控制行为

- 日志完善：记录关键操作和异常

- 测试覆盖：每个组件都要有单元测试

- 文档清晰：及时更新文档和注释

- 版本管理：使用语义化版本号

## 9.3 性能优化

- 批量操作：使用批量插入提高效率

- 并发处理：多线程/异步处理多只股票

- 缓存机制：缓存频繁访问的数据

- 索引优化：合理设计数据库索引

# 10. 部署建议

## 10.1 容器化部署

```dockerfile

FROM python:3.9-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["python", "main.py"]
```

## 10.2 环境配置

- 开发环境：SQLite + 少量股票测试

- 测试环境：MySQL + 完整功能测试

- 生产环境：PostgreSQL + 全量数据

## 10.3 监控部署

- 使用 Prometheus + Grafana 监控系统指标

- 使用 ELK Stack 进行日志分析

- 使用 Airflow 进行任务调度（可选）

# 11. 总结

本架构设计遵循了模块化、可扩展、易维护的原则，通过 ETL Pipeline 模式清晰地分离了数据处理的各个阶段。系统具有良好的扩展性，可以方便地添加新的数据类型和数据源，同时通过配置驱动的方式提供了灵活的定制能力。
> （注：文档部分内容可能由 AI 生成）